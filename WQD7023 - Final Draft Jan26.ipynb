{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb06c6b7-a4a7-4303-baf7-a0f567802d07",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff61e457-2000-46c2-b8ac-ec6f6e651e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T10:37:39.631699Z",
     "iopub.status.busy": "2026-01-28T10:37:39.630703Z",
     "iopub.status.idle": "2026-01-28T10:37:49.832707Z",
     "shell.execute_reply": "2026-01-28T10:37:49.832707Z",
     "shell.execute_reply.started": "2026-01-28T10:37:39.630703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: CUDA (seed=24)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# File: tabpfn_optuna_ensemble_all.py\n",
    "# Purpose: End-to-end TabPFN + Optuna-tuned models + Weighted Ensemble\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "# ============================================================\n",
    "# 0Ô∏è‚É£ Environment setup (one seed to rule them all)\n",
    "# ============================================================\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TABPFN_DEVICE\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "GLOBAL_SEED = 24  # üî∏ Change this ONE line to update all seeds globally\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_global_seed(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Using device: {DEVICE.upper()} (seed={GLOBAL_SEED})\")\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Load & prepare data\n",
    "# ============================================================\n",
    "\n",
    "data_path = r\"C:\\Data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df[\"RiskLevel\"] = df[\"RiskLevel\"].map({\"low risk\": 0, \"mid risk\": 1, \"high risk\": 2})\n",
    "\n",
    "X = df.drop(\"RiskLevel\", axis=1)\n",
    "y = df[\"RiskLevel\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cd7c7-05c7-43f1-b53f-2ebb125a137b",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3497a9d-d5ad-4811-818d-a8f2a56caba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:18:51.888694Z",
     "iopub.status.busy": "2026-01-28T13:18:51.887682Z",
     "iopub.status.idle": "2026-01-28T13:19:08.189348Z",
     "shell.execute_reply": "2026-01-28T13:19:08.185350Z",
     "shell.execute_reply.started": "2026-01-28T13:18:51.887682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∞ Training Baseline Models (No Tuning)...\n",
      "\n",
      "=== Baseline: RandomForest ===\n",
      "\n",
      "=== Baseline: XGBoost ===\n",
      "\n",
      "=== Baseline: LightGBM ===\n",
      "\n",
      "=== Baseline: GradientBoosting ===\n",
      "\n",
      "=== Baseline: MLP ===\n",
      "\n",
      "=== Baseline: kNN ===\n",
      "\n",
      "=== Baseline: TabPFN_GPU ===\n",
      "\n",
      "\n",
      "üèÅ === Baseline Model Performance Summary (with Time) ===\n",
      "              Model  accuracy  precision  recall  f1_macro  total_time_sec\n",
      "6        TabPFN_GPU    0.8249     0.8287  0.8271    0.8263          8.6408\n",
      "1           XGBoost    0.8101     0.8151  0.8128    0.8122          0.3495\n",
      "0      RandomForest    0.8027     0.8110  0.8067    0.8067          1.2027\n",
      "2          LightGBM    0.7965     0.8023  0.8012    0.8003          0.7340\n",
      "3  GradientBoosting    0.7719     0.7845  0.7746    0.7756          2.5407\n",
      "5               kNN    0.6497     0.6762  0.6468    0.6531          0.0349\n",
      "4               MLP    0.6140     0.6826  0.6100    0.5956          2.3457\n",
      "\n",
      "üìÅ Saved: Baseline_Model_Results_With_Time.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Baseline Models (No Tuning) + TabPFN\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüî∞ Training Baseline Models (No Tuning)...\")\n",
    "\n",
    "baseline_models = {\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=GLOBAL_SEED),\n",
    "    \"XGBoost\": XGBClassifier(random_state=GLOBAL_SEED,\n",
    "                            n_estimators=50,\n",
    "                             max_depth=5\n",
    "                        ),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=GLOBAL_SEED,\n",
    "                              verbose=-1  # silences all LightGBM info/warning logs\n",
    "                          ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=GLOBAL_SEED),\n",
    "    \"MLP\": MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=300,\n",
    "        random_state=GLOBAL_SEED,\n",
    "    ),\n",
    "    \"kNN\": KNeighborsClassifier(),\n",
    "    \"TabPFN_GPU\": TabPFNClassifier(device=DEVICE)\n",
    "}\n",
    "\n",
    "def baseline_evaluate(model_name, model, X, y, cv=5):\n",
    "    \"\"\"Evaluate a single baseline model using Stratified K-Fold with timing.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    metrics = []\n",
    "\n",
    "    print(f\"\\n=== Baseline: {model_name} ===\")\n",
    "\n",
    "    total_time = 0\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            (\"clf\", model)\n",
    "        ])\n",
    "\n",
    "        t0 = time.time()\n",
    "        pipeline.fit(X_train_fold, y_train_fold)\n",
    "        preds = pipeline.predict(X_val_fold)\n",
    "        elapsed = time.time() - t0\n",
    "        total_time += elapsed\n",
    "\n",
    "        metrics.append({\n",
    "            \"fold\": fold,\n",
    "            \"accuracy\": accuracy_score(y_val_fold, preds),\n",
    "            \"precision\": precision_score(y_val_fold, preds, average=\"macro\", zero_division=0),\n",
    "            \"recall\": recall_score(y_val_fold, preds, average=\"macro\", zero_division=0),\n",
    "            \"f1_macro\": f1_score(y_val_fold, preds, average=\"macro\", zero_division=0),\n",
    "            \"fold_time_sec\": elapsed\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    # Remove \"fold\" before averaging metrics\n",
    "    summary = df.drop(columns=[\"fold\"]).mean(numeric_only=True)\n",
    "    summary[\"Model\"] = model_name\n",
    "    summary[\"total_time_sec\"] = df[\"fold_time_sec\"].sum()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run all baselines\n",
    "# ------------------------------------------------------------\n",
    "baseline_results = []\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    result = baseline_evaluate(model_name, model, X_train, y_train)\n",
    "    baseline_results.append(result)\n",
    "\n",
    "baseline_summary = (\n",
    "    pd.DataFrame(baseline_results)\n",
    "    .sort_values(\"accuracy\", ascending=False)\n",
    "    [[\"Model\", \"accuracy\", \"precision\", \"recall\", \"f1_macro\", \"total_time_sec\"]]\n",
    ")\n",
    "\n",
    "print(\"\\n\\nüèÅ === Baseline Model Performance Summary (with Time) ===\")\n",
    "print(baseline_summary.round(4))\n",
    "\n",
    "baseline_summary.to_csv(\"Baseline_Model_Results_With_Time.csv\", index=False)\n",
    "print(\"\\nüìÅ Saved: Baseline_Model_Results_With_Time.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4775aa7-0eb8-41db-b322-b96b508841ee",
   "metadata": {},
   "source": [
    "## Phase I: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27769675-4120-4a3a-ba03-4fb13dcaab14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T03:05:39.499158Z",
     "iopub.status.busy": "2025-12-06T03:05:39.499158Z",
     "iopub.status.idle": "2025-12-06T04:51:31.019631Z",
     "shell.execute_reply": "2025-12-06T04:51:31.018628Z",
     "shell.execute_reply.started": "2025-12-06T03:05:39.499158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training TabPFN_GPU ===\n",
      "\n",
      "=== Training RandomForest ===\n",
      "\n",
      "==============================\n",
      "Model: RandomForest\n",
      "Best Parameters: {'n_estimators': 606, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "Optuna Time: 284.92 sec\n",
      "Accuracy: 0.8076\n",
      "==============================\n",
      "\n",
      "\n",
      "=== Training XGBoost ===\n",
      "\n",
      "==============================\n",
      "Model: XGBoost\n",
      "Best Parameters: {'n_estimators': 266, 'max_depth': 14, 'learning_rate': 0.03322780739006341, 'subsample': 0.8386648614390065, 'colsample_bytree': 0.794378381656277}\n",
      "Optuna Time: 325.85 sec\n",
      "Accuracy: 0.8101\n",
      "==============================\n",
      "\n",
      "\n",
      "=== Training LightGBM ===\n",
      "\n",
      "==============================\n",
      "Model: LightGBM\n",
      "Best Parameters: {'n_estimators': 526, 'max_depth': 11, 'learning_rate': 0.1702355797771671, 'num_leaves': 46}\n",
      "Optuna Time: 184.07 sec\n",
      "Accuracy: 0.8027\n",
      "==============================\n",
      "\n",
      "\n",
      "=== Training GradientBoosting ===\n",
      "\n",
      "==============================\n",
      "Model: GradientBoosting\n",
      "Best Parameters: {'n_estimators': 346, 'learning_rate': 0.11163786742578959, 'max_depth': 14}\n",
      "Optuna Time: 3892.46 sec\n",
      "Accuracy: 0.8014\n",
      "==============================\n",
      "\n",
      "\n",
      "=== Training MLP ===\n",
      "\n",
      "==============================\n",
      "Model: MLP\n",
      "Best Parameters: {'hidden_layer_sizes': (265, 89), 'activation': 'tanh', 'learning_rate_init': 0.0058985097017337715, 'max_iter': 400, 'random_state': 24}\n",
      "Optuna Time: 1583.13 sec\n",
      "Accuracy: 0.7595\n",
      "==============================\n",
      "\n",
      "\n",
      "=== Training kNN ===\n",
      "\n",
      "==============================\n",
      "Model: kNN\n",
      "Best Parameters: {'n_neighbors': 20, 'weights': 'distance', 'p': 1}\n",
      "Optuna Time: 5.35 sec\n",
      "Accuracy: 0.7854\n",
      "==============================\n",
      "\n",
      "\n",
      "üèÅ === Model Summary ===\n",
      "              Model  accuracy  f1_macro\n",
      "0        TabPFN_GPU    0.8249    0.8263\n",
      "2           XGBoost    0.8101    0.8126\n",
      "1      RandomForest    0.8076    0.8121\n",
      "3          LightGBM    0.8027    0.8066\n",
      "4  GradientBoosting    0.8014    0.8044\n",
      "6               kNN    0.7854    0.7898\n",
      "5               MLP    0.7595    0.7635\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2Ô∏è‚É£ Define hyperparameter spaces\n",
    "# ============================================================\n",
    "\n",
    "def rf_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 800),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 40),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 15),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "    }\n",
    "\n",
    "def xgb_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"use_label_encoder\": False,\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "    }\n",
    "\n",
    "def lgbm_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 150),\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "def gb_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 800),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "    }\n",
    "\n",
    "def mlp_space(trial):\n",
    "    return {\n",
    "        \"hidden_layer_sizes\": (\n",
    "            trial.suggest_int(\"h1\", 50, 300),\n",
    "            trial.suggest_int(\"h2\", 50, 300),\n",
    "        ),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        \"learning_rate_init\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        \"max_iter\": 400,\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "    }\n",
    "\n",
    "def knn_space(trial):\n",
    "    return {\n",
    "        \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 3, 30),\n",
    "        \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "        \"p\": trial.suggest_categorical(\"p\", [1, 2]),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ Evaluation helper\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(model_name, model, X, y, cv=5):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    records = []\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "        t0 = time.time()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        t1 = time.time() - t0\n",
    "        records.append({\n",
    "            \"fold\": fold,\n",
    "            \"accuracy\": accuracy_score(y_val, preds),\n",
    "            \"precision\": precision_score(y_val, preds, average=\"macro\", zero_division=0),\n",
    "            \"recall\": recall_score(y_val, preds, average=\"macro\", zero_division=0),\n",
    "            \"f1_macro\": f1_score(y_val, preds, average=\"macro\", zero_division=0),\n",
    "            \"train_time_sec\": t1\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "    summary = df.mean().round(4)\n",
    "    summary[\"Model\"] = model_name\n",
    "    return summary\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ Optuna tuning\n",
    "# ============================================================\n",
    "\n",
    "def optuna_tune(model_name, model_class, param_space, X, y, n_trials=30):\n",
    "    \"\"\"\n",
    "    Tune model hyperparameters with Optuna and print:\n",
    "      - best parameters\n",
    "      - total optimization time (seconds)\n",
    "      - accuracy and F1 score\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # measure tuning duration\n",
    "\n",
    "    def objective(trial):\n",
    "        params = param_space(trial)\n",
    "        model = model_class(**params)\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", model)])\n",
    "        scores = cross_val_score(pipe, X, y, cv=5, scoring=\"accuracy\")\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "    elapsed = round(time.time() - start_time, 2)\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Fix MLP special case\n",
    "    if model_name == \"MLP\":\n",
    "        best_params = {\n",
    "            \"hidden_layer_sizes\": (best_params[\"h1\"], best_params[\"h2\"]),\n",
    "            \"activation\": best_params[\"activation\"],\n",
    "            \"learning_rate_init\": best_params[\"lr\"],\n",
    "            \"max_iter\": 400,\n",
    "            \"random_state\": GLOBAL_SEED,\n",
    "        }\n",
    "\n",
    "    # Save tuning curve\n",
    "    pd.DataFrame({\n",
    "        \"trial\": [t.number for t in study.trials],\n",
    "        \"accuracy\": [t.value for t in study.trials],\n",
    "    }).to_csv(f\"{model_name}_optuna_curve.csv\", index=False)\n",
    "\n",
    "    # Train and evaluate with best params\n",
    "    best_model = model_class(**best_params)\n",
    "    final_pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", best_model)])\n",
    "    summary = evaluate_model(model_name, final_pipeline, X, y)\n",
    "\n",
    "    # Extract accuracy and F1 from your summary\n",
    "    acc = summary.get(\"accuracy\")\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Optuna Time: {elapsed} sec\")\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    return summary, best_params\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ Train TabPFN and tune all other models\n",
    "# ============================================================\n",
    "\n",
    "results, best_params_dict = [], {}\n",
    "tabpfn_model = TabPFNClassifier(device=DEVICE)\n",
    "tabpfn_summary = evaluate_model(\"TabPFN_GPU\", tabpfn_model, X_train, y_train)\n",
    "tabpfn_summary[\"optuna_time_min\"] = 0.0\n",
    "results.append(tabpfn_summary)\n",
    "best_params_dict[\"TabPFN_GPU\"] = {\"device\": DEVICE}\n",
    "n_trials_tune = 100\n",
    "\n",
    "tunable_models = {\n",
    "    \"RandomForest\": (RandomForestClassifier, rf_space, n_trials_tune),\n",
    "    \"XGBoost\": (XGBClassifier, xgb_space, n_trials_tune),\n",
    "    \"LightGBM\": (LGBMClassifier, lgbm_space, n_trials_tune),\n",
    "    \"GradientBoosting\": (GradientBoostingClassifier, gb_space, n_trials_tune),\n",
    "    \"MLP\": (MLPClassifier, mlp_space, n_trials_tune),\n",
    "    \"kNN\": (KNeighborsClassifier, knn_space, n_trials_tune),\n",
    "}\n",
    "\n",
    "for name, (cls, space, n_trials) in tunable_models.items():\n",
    "    summary, params = optuna_tune(name, cls, space, X_train, y_train, n_trials)\n",
    "    results.append(summary)\n",
    "    best_params_dict[name] = params\n",
    "\n",
    "final_summary = pd.DataFrame(results).sort_values(by=\"accuracy\", ascending=False)\n",
    "final_summary.to_csv(\"All_Model_Results.csv\", index=False)\n",
    "\n",
    "print(\"\\nüèÅ === Model Summary ===\")\n",
    "print(final_summary[[\"Model\", \"accuracy\", \"f1_macro\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06df41-828b-4ac4-a880-9fdbf4255c2f",
   "metadata": {},
   "source": [
    "## Phase II: Weighted Soft-voting Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66f13799-61c4-4fc0-bdf8-b8e2fb08a953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T07:44:18.572904Z",
     "iopub.status.busy": "2025-12-06T07:44:18.571906Z",
     "iopub.status.idle": "2025-12-06T07:44:32.467246Z",
     "shell.execute_reply": "2025-12-06T07:44:32.466246Z",
     "shell.execute_reply.started": "2025-12-06T07:44:18.572904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ù Building Multiple Weighted Ensembles (Top-3 + TabPFN variations)‚Ä¶\n",
      "\n",
      "\n",
      "üèÜ === Ensemble Comparison (with model names + timing) ===\n",
      "         Ensemble                                       Models  Accuracy  \\\n",
      "0  Top-3 + TabPFN  XGBoost, RandomForest, LightGBM, TabPFN_GPU    0.8867   \n",
      "1      Top-3 only              XGBoost, RandomForest, LightGBM    0.8768   \n",
      "2   TabPFN + Best                          XGBoost, TabPFN_GPU    0.8916   \n",
      "3    TabPFN + 2nd                     RandomForest, TabPFN_GPU    0.8818   \n",
      "4    TabPFN + 3rd                         LightGBM, TabPFN_GPU    0.8818   \n",
      "\n",
      "   Precision  Recall      F1  Train_Time  Test_Time  Total_Time  \n",
      "0     0.8858  0.8946  0.8889      2.3179     1.7089      4.0268  \n",
      "1     0.8770  0.8863  0.8799      2.0206     0.0868      2.1074  \n",
      "2     0.8922  0.8987  0.8947      0.8948     1.4271      2.3219  \n",
      "3     0.8805  0.8887  0.8836      1.6542     1.4573      3.1115  \n",
      "4     0.8821  0.8905  0.8841      0.8202     1.4272      2.2474  \n",
      "\n",
      "üìÅ Saved: Ensemble_Comparison.csv\n",
      "\n",
      "üìÅ Saved: Ensemble_Top3_TabPFN_Predictions.csv\n",
      "‚úÖ All ensemble variations completed (with timing).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ Weighted Ensemble: Multiple Configurations + Comparison\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nü§ù Building Multiple Weighted Ensembles (Top-3 + TabPFN variations)‚Ä¶\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: build a timed weighted ensemble\n",
    "# ------------------------------------------------------------\n",
    "def run_weighted_ensemble(model_entries, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    model_entries = list of tuples:\n",
    "       [ (name, model_instance, weight), ... ]\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # TIMING ‚Äî TRAINING\n",
    "    # -------------------------\n",
    "    t0_train = time.time()\n",
    "\n",
    "    trained = []\n",
    "    weights = []\n",
    "\n",
    "    for name, model, w in model_entries:\n",
    "        model.fit(X_train, y_train)\n",
    "        trained.append((name, model))\n",
    "        weights.append(w)\n",
    "\n",
    "    train_time = time.time() - t0_train\n",
    "\n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # -------------------------\n",
    "    # TIMING ‚Äî TESTING\n",
    "    # -------------------------\n",
    "    t0_test = time.time()\n",
    "\n",
    "    proba_sum = np.zeros((X_test.shape[0], len(np.unique(y_test))))\n",
    "    for (name, model), w in zip(trained, weights):\n",
    "        proba_sum += model.predict_proba(X_test) * w\n",
    "\n",
    "    pred = np.argmax(proba_sum, axis=1)\n",
    "\n",
    "    test_time = time.time() - t0_test\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision\": precision_score(y_test, pred, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_test, pred, average=\"macro\"),\n",
    "        \"f1\": f1_score(y_test, pred, average=\"macro\"),\n",
    "        \"pred\": pred,\n",
    "        \"train_time\": train_time,\n",
    "        \"test_time\": test_time,\n",
    "        \"total_time\": train_time + test_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Prep: Sorted tuned models & top-3 (EXCLUDING TabPFN)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Remove TabPFN from ranking so it never appears in \"Top-3\"\n",
    "tuned_non_tabpfn = final_summary[final_summary[\"Model\"] != \"TabPFN_GPU\"]\n",
    "\n",
    "# Sort by accuracy\n",
    "tuned_non_tabpfn = tuned_non_tabpfn.sort_values(\"accuracy\", ascending=False)\n",
    "\n",
    "# Select the top-3 non-TabPFN models\n",
    "top3 = tuned_non_tabpfn.head(3)\n",
    "\n",
    "\n",
    "model_map = {\n",
    "    \"RandomForest\": RandomForestClassifier,\n",
    "    \"XGBoost\": XGBClassifier,\n",
    "    \"LightGBM\": LGBMClassifier,\n",
    "    \"GradientBoosting\": GradientBoostingClassifier,\n",
    "    \"MLP\": MLPClassifier,\n",
    "    \"kNN\": KNeighborsClassifier,\n",
    "}\n",
    "\n",
    "def build_model_entry(row):\n",
    "    name = row[\"Model\"]\n",
    "\n",
    "    # TabPFN should not be rebuilt ‚Äî use the pre-trained instance\n",
    "    if name == \"TabPFN_GPU\":\n",
    "        return (\"TabPFN_GPU\", tabpfn_model, row[\"accuracy\"])\n",
    "\n",
    "    params = best_params_dict[name]\n",
    "    model = model_map[name](**params)\n",
    "    return (name, model, row[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Ensemble 1: Top-3 + TabPFN\n",
    "# ------------------------------------------------------------\n",
    "tabpfn_acc = tuned_models.loc[tuned_models[\"Model\"] == \"TabPFN_GPU\", \"accuracy\"].values[0]\n",
    "top3_plus_tabpfn = [build_model_entry(row) for _, row in top3.iterrows()] + [\n",
    "    (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)\n",
    "]\n",
    "\n",
    "res_top3_tabpfn = run_weighted_ensemble(top3_plus_tabpfn, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Ensemble 2: Top-3 only (no TabPFN)\n",
    "# ------------------------------------------------------------\n",
    "top3_only = [build_model_entry(row) for _, row in top3.iterrows()]\n",
    "\n",
    "res_top3_only = run_weighted_ensemble(top3_only, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Ensembles 3‚Äì5: TabPFN + each of the top-3 individually\n",
    "# ------------------------------------------------------------\n",
    "res_tabpfn_best = run_weighted_ensemble(\n",
    "    [build_model_entry(top3.iloc[0]), (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)],\n",
    "    X_train, y_train, X_test, y_test,\n",
    ")\n",
    "\n",
    "res_tabpfn_second = run_weighted_ensemble(\n",
    "    [build_model_entry(top3.iloc[1]), (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)],\n",
    "    X_train, y_train, X_test, y_test,\n",
    ")\n",
    "\n",
    "res_tabpfn_third = run_weighted_ensemble(\n",
    "    [build_model_entry(top3.iloc[2]), (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)],\n",
    "    X_train, y_train, X_test, y_test,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üìä Comparison Table WITH MODEL NAMES + TIMING\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def extract_model_names(model_entries):\n",
    "    \"\"\"Return comma-separated model names from ensemble entries.\"\"\"\n",
    "    return \", \".join([name for name, _, _ in model_entries])\n",
    "\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    [\n",
    "        \"Top-3 + TabPFN\",\n",
    "        extract_model_names(top3_plus_tabpfn),\n",
    "        res_top3_tabpfn[\"accuracy\"],\n",
    "        res_top3_tabpfn[\"precision\"],\n",
    "        res_top3_tabpfn[\"recall\"],\n",
    "        res_top3_tabpfn[\"f1\"],\n",
    "        res_top3_tabpfn[\"train_time\"],\n",
    "        res_top3_tabpfn[\"test_time\"],\n",
    "        res_top3_tabpfn[\"total_time\"]\n",
    "    ],\n",
    "    [\n",
    "        \"Top-3 only\",\n",
    "        extract_model_names(top3_only),\n",
    "        res_top3_only[\"accuracy\"],\n",
    "        res_top3_only[\"precision\"],\n",
    "        res_top3_only[\"recall\"],\n",
    "        res_top3_only[\"f1\"],\n",
    "        res_top3_only[\"train_time\"],\n",
    "        res_top3_only[\"test_time\"],\n",
    "        res_top3_only[\"total_time\"]\n",
    "    ],\n",
    "    [\n",
    "        \"TabPFN + Best\",\n",
    "        extract_model_names([build_model_entry(top3.iloc[0]), (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)]),\n",
    "        res_tabpfn_best[\"accuracy\"],\n",
    "        res_tabpfn_best[\"precision\"],\n",
    "        res_tabpfn_best[\"recall\"],\n",
    "        res_tabpfn_best[\"f1\"],\n",
    "        res_tabpfn_best[\"train_time\"],\n",
    "        res_tabpfn_best[\"test_time\"],\n",
    "        res_tabpfn_best[\"total_time\"]\n",
    "    ],\n",
    "    [\n",
    "        \"TabPFN + 2nd\",\n",
    "        extract_model_names([build_model_entry(top3.iloc[1]), (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)]),\n",
    "        res_tabpfn_second[\"accuracy\"],\n",
    "        res_tabpfn_second[\"precision\"],\n",
    "        res_tabpfn_second[\"recall\"],\n",
    "        res_tabpfn_second[\"f1\"],\n",
    "        res_tabpfn_second[\"train_time\"],\n",
    "        res_tabpfn_second[\"test_time\"],\n",
    "        res_tabpfn_second[\"total_time\"]\n",
    "    ],\n",
    "    [\n",
    "        \"TabPFN + 3rd\",\n",
    "        extract_model_names([build_model_entry(top3.iloc[2]), (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc)]),\n",
    "        res_tabpfn_third[\"accuracy\"],\n",
    "        res_tabpfn_third[\"precision\"],\n",
    "        res_tabpfn_third[\"recall\"],\n",
    "        res_tabpfn_third[\"f1\"],\n",
    "        res_tabpfn_third[\"train_time\"],\n",
    "        res_tabpfn_third[\"test_time\"],\n",
    "        res_tabpfn_third[\"total_time\"]\n",
    "    ],\n",
    "], columns=[\n",
    "    \"Ensemble\", \"Models\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\",\n",
    "    \"Train_Time\", \"Test_Time\", \"Total_Time\"\n",
    "])\n",
    "\n",
    "print(\"\\n\\nüèÜ === Ensemble Comparison (with model names + timing) ===\")\n",
    "print(comparison.round(4))\n",
    "\n",
    "comparison.to_csv(\"Ensemble_Comparison.csv\", index=False)\n",
    "print(\"\\nüìÅ Saved: Ensemble_Comparison.csv\")\n",
    "\n",
    "# Save main ensemble predictions (Top-3 + TabPFN)\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": res_top3_tabpfn[\"pred\"]\n",
    "}).to_csv(\"Ensemble_Top3_TabPFN_Predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìÅ Saved: Ensemble_Top3_TabPFN_Predictions.csv\")\n",
    "print(\"‚úÖ All ensemble variations completed (with timing).\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab869b-edca-42f6-b6e6-7cfcfa4b035d",
   "metadata": {},
   "source": [
    "## Phase II: Stacking Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db41bb68-977c-450a-a533-a21f730c9d52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T08:00:53.806945Z",
     "iopub.status.busy": "2025-12-06T08:00:53.806945Z",
     "iopub.status.idle": "2025-12-06T08:02:10.551050Z",
     "shell.execute_reply": "2025-12-06T08:02:10.549646Z",
     "shell.execute_reply.started": "2025-12-06T08:00:53.806945Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 models (excluding TabPFN): ['XGBoost', 'RandomForest', 'LightGBM']\n",
      "\n",
      "\n",
      "üèÜ === ALL ENSEMBLES RANKED BY ACCURACY ===\n",
      "                       Ensemble  \\\n",
      "2                 TabPFN + Best   \n",
      "0                Top-3 + TabPFN   \n",
      "3                  TabPFN + 2nd   \n",
      "4                  TabPFN + 3rd   \n",
      "1                    Top-3 only   \n",
      "8      Stacking (meta=LightGBM)   \n",
      "5    Stacking (meta=TabPFN_GPU)   \n",
      "6       Stacking (meta=XGBoost)   \n",
      "7  Stacking (meta=RandomForest)   \n",
      "\n",
      "                                              Models  Accuracy  Precision  \\\n",
      "2                                XGBoost, TabPFN_GPU    0.8916     0.8922   \n",
      "0        XGBoost, RandomForest, LightGBM, TabPFN_GPU    0.8867     0.8858   \n",
      "3                           RandomForest, TabPFN_GPU    0.8818     0.8805   \n",
      "4                               LightGBM, TabPFN_GPU    0.8818     0.8821   \n",
      "1                    XGBoost, RandomForest, LightGBM    0.8768     0.8770   \n",
      "8  XGBoost_base, RandomForest_base, TabPFN_GPU_ba...    0.8621     0.8636   \n",
      "5  XGBoost_base, RandomForest_base, LightGBM_base...    0.8571     0.8557   \n",
      "6  RandomForest_base, LightGBM_base, TabPFN_GPU_b...    0.8571     0.8580   \n",
      "7  XGBoost_base, LightGBM_base, TabPFN_GPU_base ‚Üí...    0.8522     0.8510   \n",
      "\n",
      "   Recall      F1  Train_Time  Test_Time  Total_Time  \n",
      "2  0.8987  0.8947      0.8948     1.4271      2.3219  \n",
      "0  0.8946  0.8889      2.3179     1.7089      4.0268  \n",
      "3  0.8887  0.8836      1.6542     1.4573      3.1115  \n",
      "4  0.8905  0.8841      0.8202     1.4272      2.2474  \n",
      "1  0.8863  0.8799      2.0206     0.0868      2.1074  \n",
      "8  0.8660  0.8641     19.8432     1.5210     21.3642  \n",
      "5  0.8617  0.8582     12.9107     2.1623     15.0730  \n",
      "6  0.8621  0.8569     21.2881     1.4614     22.7495  \n",
      "7  0.8559  0.8525     15.8457     1.5014     17.3471  \n",
      "\n",
      "\n",
      "üèÜ === ALL ENSEMBLES RANKED BY F1 SCORE ===\n",
      "                       Ensemble  \\\n",
      "2                 TabPFN + Best   \n",
      "0                Top-3 + TabPFN   \n",
      "4                  TabPFN + 3rd   \n",
      "3                  TabPFN + 2nd   \n",
      "1                    Top-3 only   \n",
      "8      Stacking (meta=LightGBM)   \n",
      "5    Stacking (meta=TabPFN_GPU)   \n",
      "6       Stacking (meta=XGBoost)   \n",
      "7  Stacking (meta=RandomForest)   \n",
      "\n",
      "                                              Models  Accuracy  Precision  \\\n",
      "2                                XGBoost, TabPFN_GPU    0.8916     0.8922   \n",
      "0        XGBoost, RandomForest, LightGBM, TabPFN_GPU    0.8867     0.8858   \n",
      "4                               LightGBM, TabPFN_GPU    0.8818     0.8821   \n",
      "3                           RandomForest, TabPFN_GPU    0.8818     0.8805   \n",
      "1                    XGBoost, RandomForest, LightGBM    0.8768     0.8770   \n",
      "8  XGBoost_base, RandomForest_base, TabPFN_GPU_ba...    0.8621     0.8636   \n",
      "5  XGBoost_base, RandomForest_base, LightGBM_base...    0.8571     0.8557   \n",
      "6  RandomForest_base, LightGBM_base, TabPFN_GPU_b...    0.8571     0.8580   \n",
      "7  XGBoost_base, LightGBM_base, TabPFN_GPU_base ‚Üí...    0.8522     0.8510   \n",
      "\n",
      "   Recall      F1  Train_Time  Test_Time  Total_Time  \n",
      "2  0.8987  0.8947      0.8948     1.4271      2.3219  \n",
      "0  0.8946  0.8889      2.3179     1.7089      4.0268  \n",
      "4  0.8905  0.8841      0.8202     1.4272      2.2474  \n",
      "3  0.8887  0.8836      1.6542     1.4573      3.1115  \n",
      "1  0.8863  0.8799      2.0206     0.0868      2.1074  \n",
      "8  0.8660  0.8641     19.8432     1.5210     21.3642  \n",
      "5  0.8617  0.8582     12.9107     2.1623     15.0730  \n",
      "6  0.8621  0.8569     21.2881     1.4614     22.7495  \n",
      "7  0.8559  0.8525     15.8457     1.5014     17.3471  \n",
      "\n",
      "\n",
      "üìä === BEST MODEL (Accuracy): TabPFN + Best ===\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_pred' parameter of classification_report must be an array-like or a sparse matrix. Got None instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    155\u001b[39m     y_pred_best_acc = res_top3_tabpfn[\u001b[33m\"\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m best_acc_name == \u001b[33m\"\u001b[39m\u001b[33mTop-3 + TabPFN\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä === BEST MODEL (Accuracy):\u001b[39m\u001b[33m\"\u001b[39m, best_acc_name, \u001b[33m\"\u001b[39m\u001b[33m===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_best_acc\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:208\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m to_ignore += [\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    206\u001b[39m params = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.arguments.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__qualname__\u001b[39;49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'y_pred' parameter of classification_report must be an array-like or a sparse matrix. Got None instead."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7Ô∏è‚É£ STACKING ENSEMBLES (With Timing + Model Listing)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import time\n",
    "\n",
    "# ------------------------------------\n",
    "# Ensure top-3 DOES NOT include TabPFN\n",
    "# ------------------------------------\n",
    "top3_no_tabpfn = tuned_models[tuned_models[\"Model\"] != \"TabPFN_GPU\"].head(3)\n",
    "top3_names = top3_no_tabpfn[\"Model\"].tolist()\n",
    "\n",
    "print(\"Top-3 models (excluding TabPFN):\", top3_names)\n",
    "\n",
    "def build_model_instance(name):\n",
    "    if name == \"TabPFN_GPU\":\n",
    "        return tabpfn_model\n",
    "    return model_map[name](**best_params_dict[name])\n",
    "\n",
    "stacking_results = {}\n",
    "\n",
    "def train_stacking(meta_name):\n",
    "    \n",
    "    meta_model = build_model_instance(meta_name)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Base models = top-3 + TabPFN EXCEPT meta\n",
    "    # ------------------------------------\n",
    "    all_models = top3_names + [\"TabPFN_GPU\"]\n",
    "    base_models = [m for m in all_models if m != meta_name]\n",
    "    \n",
    "    # Remove duplicates (safety)\n",
    "    base_models = list(dict.fromkeys(base_models))\n",
    "\n",
    "    # Unique (name, estimator) pairs for sklearn\n",
    "    estimators = [(f\"{m}_base\", build_model_instance(m)) for m in base_models]\n",
    "\n",
    "    # ------------------------------------\n",
    "    # TIMING ‚Äî TRAINING\n",
    "    # ------------------------------------\n",
    "    t0_train = time.time()\n",
    "    stack = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta_model,\n",
    "        stack_method=\"predict_proba\",\n",
    "        passthrough=False\n",
    "    )\n",
    "    stack.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0_train\n",
    "\n",
    "    # ------------------------------------\n",
    "    # TIMING ‚Äî TESTING\n",
    "    # ------------------------------------\n",
    "    t0_test = time.time()\n",
    "    preds = stack.predict(X_test)\n",
    "    test_time = time.time() - t0_test\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Return full record\n",
    "    # ------------------------------------\n",
    "    return {\n",
    "        \"pred\": preds,\n",
    "        \"accuracy\": accuracy_score(y_test, preds),\n",
    "        \"precision\": precision_score(y_test, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_test, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(y_test, preds, average=\"macro\"),\n",
    "        \"meta\": meta_name,\n",
    "        \"base_models\": base_models,\n",
    "        \"train_time\": train_time,\n",
    "        \"test_time\": test_time,\n",
    "        \"total_time\": train_time + test_time\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Train stacking ensembles\n",
    "# ------------------------------------\n",
    "for meta in [\"TabPFN_GPU\"] + top3_names:\n",
    "    stacking_results[meta] = train_stacking(meta)\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Convert to DataFrame with clean ‚ÄúModels‚Äù column\n",
    "# ------------------------------------\n",
    "def join_models(meta, base):\n",
    "    \"\"\"\n",
    "    Represent ensemble structure clearly:\n",
    "    Example:\n",
    "        RF_base, XGB_base, TabPFN_base ‚Üí META=LightGBM\n",
    "    \"\"\"\n",
    "    base_list = [f\"{b}_base\" for b in base]\n",
    "    base_str = \", \".join(base_list)\n",
    "    return f\"{base_str} ‚Üí META = {meta}\"\n",
    "\n",
    "stacking_df = pd.DataFrame([\n",
    "    [\n",
    "        f\"Stacking (meta={meta})\",\n",
    "        stacking_results[meta][\"accuracy\"],\n",
    "        stacking_results[meta][\"precision\"],\n",
    "        stacking_results[meta][\"recall\"],\n",
    "        stacking_results[meta][\"f1\"],\n",
    "        stacking_results[meta][\"train_time\"],\n",
    "        stacking_results[meta][\"test_time\"],\n",
    "        stacking_results[meta][\"total_time\"],\n",
    "        join_models(meta, stacking_results[meta][\"base_models\"])\n",
    "    ]\n",
    "    for meta in stacking_results\n",
    "], columns=[\n",
    "    \"Ensemble\",\n",
    "    \"Accuracy\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"Train_Time\",\n",
    "    \"Test_Time\",\n",
    "    \"Total_Time\",\n",
    "    \"Models\"\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8Ô∏è‚É£ COMBINE ALL ENSEMBLES (Voting + Stacking)\n",
    "# ============================================================\n",
    "\n",
    "all_ensembles = pd.concat([comparison, stacking_df], ignore_index=True)\n",
    "\n",
    "# Sort two ways\n",
    "rank_accuracy = all_ensembles.sort_values(\"Accuracy\", ascending=False)\n",
    "rank_f1 = all_ensembles.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "print(\"\\n\\nüèÜ === ALL ENSEMBLES RANKED BY ACCURACY ===\")\n",
    "print(rank_accuracy.round(4))\n",
    "\n",
    "print(\"\\n\\nüèÜ === ALL ENSEMBLES RANKED BY F1 SCORE ===\")\n",
    "print(rank_f1.round(4))\n",
    "\n",
    "rank_accuracy.to_csv(\"All_Ensembles_Sorted_by_Accuracy.csv\", index=False)\n",
    "rank_f1.to_csv(\"All_Ensembles_Sorted_by_F1.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c005bc-17a8-4ed8-a345-9c3eb9bc9957",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "## Final Ensemble Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5089feec-daa0-41f4-ab03-5f4d52c9831d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:20:20.539040Z",
     "iopub.status.busy": "2025-12-06T10:20:20.538043Z",
     "iopub.status.idle": "2025-12-06T10:20:23.588800Z",
     "shell.execute_reply": "2025-12-06T10:20:23.587799Z",
     "shell.execute_reply.started": "2025-12-06T10:20:20.539040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training FINAL MODEL: TabPFN + XGBoost (Weighted Ensemble)‚Ä¶\n",
      "\n",
      "üìä === FINAL MODEL: TabPFN + XGBoost (Weighted Ensemble) ===\n",
      "\n",
      "              precision  recall  f1-score   support\n",
      "0                0.9200  0.8519    0.8846   81.0000\n",
      "1                0.8429  0.8806    0.8613   67.0000\n",
      "2                0.9138  0.9636    0.9381   55.0000\n",
      "accuracy         0.8916  0.8916    0.8916    0.8916\n",
      "macro avg        0.8922  0.8987    0.8947  203.0000\n",
      "weighted avg     0.8929  0.8916    0.8914  203.0000\n",
      "\n",
      "‚è±Ô∏è === TIMING (Final Model) ===\n",
      "Training Time : 1.2174 sec\n",
      "Testing Time  : 1.7763 sec\n",
      "Total Time    : 2.9937 sec\n",
      "\n",
      "üìÅ Saved predictions to: Final_Model_TabPFN_XGBoost_Predictions.csv\n",
      "üéâ Final model training completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ‚≠ê FINAL MODEL: TabPFN + XGBoost Weighted Ensemble\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nüöÄ Training FINAL MODEL: TabPFN + XGBoost (Weighted Ensemble)‚Ä¶\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Get accuracy scores for weighting\n",
    "# ------------------------------------------------------------\n",
    "tabpfn_acc = float(tuned_models.loc[tuned_models[\"Model\"] == \"TabPFN_GPU\", \"accuracy\"].values[0])\n",
    "xgb_acc = float(final_summary.loc[final_summary[\"Model\"] == \"XGBoost\", \"accuracy\"].values[0])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build XGBoost model\n",
    "# ------------------------------------------------------------\n",
    "xgb_model = XGBClassifier(**best_params_dict[\"XGBoost\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Prepare weighted models\n",
    "# ------------------------------------------------------------\n",
    "final_models = [\n",
    "    (\"XGBoost\", xgb_model, xgb_acc),\n",
    "    (\"TabPFN_GPU\", tabpfn_model, tabpfn_acc),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run final ensemble\n",
    "# ------------------------------------------------------------\n",
    "final_result = run_weighted_ensemble(\n",
    "    final_models, X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "y_pred_final = final_result[\"pred\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Print classification report (rounded)\n",
    "# ------------------------------------------------------------\n",
    "report = classification_report(y_test, y_pred_final, output_dict=True)\n",
    "report_df = pd.DataFrame(report).T.round(4)\n",
    "\n",
    "print(\"\\nüìä === FINAL MODEL: TabPFN + XGBoost (Weighted Ensemble) ===\\n\")\n",
    "print(report_df)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Print timing (rounded)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n‚è±Ô∏è === TIMING (Final Model) ===\")\n",
    "print(f\"Training Time : {final_result['train_time']:.4f} sec\")\n",
    "print(f\"Testing Time  : {final_result['test_time']:.4f} sec\")\n",
    "print(f\"Total Time    : {final_result['total_time']:.4f} sec\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save predictions\n",
    "# ------------------------------------------------------------\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": y_pred_final\n",
    "}).to_csv(\"Final_Model_TabPFN_XGBoost_Predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìÅ Saved predictions to: Final_Model_TabPFN_XGBoost_Predictions.csv\")\n",
    "print(\"üéâ Final model training completed.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27fa4f-ab99-440d-a99c-7f5c4846b293",
   "metadata": {},
   "source": [
    "# Save Final Model for Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605ebc0-25e1-49f7-8e4a-2fcd0662e501",
   "metadata": {},
   "source": [
    "## Save TabPFN and XGBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b026d31-34cb-477c-a34c-16849aebf155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:10:02.846850Z",
     "iopub.status.busy": "2025-12-06T10:10:02.832841Z",
     "iopub.status.idle": "2025-12-06T10:10:08.587779Z",
     "shell.execute_reply": "2025-12-06T10:10:08.491038Z",
     "shell.execute_reply.started": "2025-12-06T10:10:02.846850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved XGBoost model: final_xgb_model.pkl\n",
      "‚úÖ Saved TabPFN model: final_tabpfn_model.pkl\n",
      "‚úÖ Saved ensemble weights: final_ensemble_weights.json\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# -----------------------------\n",
    "# Already trained models\n",
    "# -----------------------------\n",
    "# xgb_model is trained with X_train, y_train\n",
    "# tabpfn_model is already trained earlier\n",
    "\n",
    "# Save XGBoost\n",
    "joblib.dump(xgb_model, \"final_xgb_model.pkl\")\n",
    "print(\"‚úÖ Saved XGBoost model: final_xgb_model.pkl\")\n",
    "\n",
    "# Save TabPFN\n",
    "joblib.dump(tabpfn_model, \"final_tabpfn_model.pkl\")\n",
    "print(\"‚úÖ Saved TabPFN model: final_tabpfn_model.pkl\")\n",
    "\n",
    "# Optional: save ensemble weights for reference\n",
    "import json\n",
    "weights = {\n",
    "    \"XGBoost\": float(xgb_acc),\n",
    "    \"TabPFN_GPU\": float(tabpfn_acc)\n",
    "}\n",
    "with open(\"final_ensemble_weights.json\", \"w\") as f:\n",
    "    json.dump(weights, f)\n",
    "print(\"‚úÖ Saved ensemble weights: final_ensemble_weights.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5a27d-ae8a-4f81-ba82-315962d37cdc",
   "metadata": {},
   "source": [
    "# Dashboard Building with Gradio\n",
    "## Pre-compute SHAP Values with k-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b6723-cd59-4ee4-9412-02b9c5cd98c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T08:58:49.316093Z",
     "iopub.status.busy": "2025-12-06T08:58:49.315097Z",
     "iopub.status.idle": "2025-12-06T08:58:49.322416Z",
     "shell.execute_reply": "2025-12-06T08:58:49.321415Z",
     "shell.execute_reply.started": "2025-12-06T08:58:49.316093Z"
    }
   },
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279dbc1-2684-41d1-8e49-3e266cf646e7",
   "metadata": {},
   "source": [
    "### V1: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200828e8-a2ee-497d-8ab3-bc41da2d2ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T12:02:38.818344Z",
     "iopub.status.busy": "2026-01-28T12:02:38.818344Z",
     "iopub.status.idle": "2026-01-28T12:02:39.990682Z",
     "shell.execute_reply": "2026-01-28T12:02:39.989679Z",
     "shell.execute_reply.started": "2026-01-28T12:02:38.818344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import torch\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"Age\",\n",
    "    \"Systolic BP\",\n",
    "    \"Diastolic BP\",\n",
    "    \"Blood Sugar (mmol/L)\",\n",
    "    \"Temperature (¬∞F)\",\n",
    "    \"Heart Rate\",\n",
    "]\n",
    "\n",
    "RISK_MAP = {0: \"Low Risk\", 1: \"Mid Risk\", 2: \"High Risk\"}\n",
    "\n",
    "def format_input(age, sbp, dbp, sugar, temp, hr):\n",
    "    return np.array([[age, sbp, dbp, sugar, temp, hr]], dtype=float)\n",
    "\n",
    "def timed(fn):\n",
    "    start = time.perf_counter()\n",
    "    out = fn()\n",
    "    return out, (time.perf_counter() - start) * 1000\n",
    "\n",
    "model = joblib.load(\"final_xgb_model.pkl\")\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "def predict(age, sbp, dbp, sugar, temp, hr):\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "    risk, t_pred = timed(lambda: RISK_MAP[int(model.predict(X)[0])])\n",
    "\n",
    "    metrics = (\n",
    "        \"Model Accuracy: 81.01%\\n\"\n",
    "        \"F1 Score: 81.26%\\n\"\n",
    "        f\"Inference Time: {t_pred:.2f} ms\"\n",
    "    )\n",
    "\n",
    "    return risk, metrics, f\"{t_pred:.2f} ms\", None, \"Calculating...\"\n",
    "\n",
    "def compute_shap(age, sbp, dbp, sugar, temp, hr):\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "\n",
    "    def _compute():\n",
    "        shap_vals = explainer.shap_values(X)\n",
    "        pred_class = int(model.predict(X)[0])\n",
    "\n",
    "        if isinstance(shap_vals, list):\n",
    "            values = shap_vals[pred_class][0]\n",
    "        else:\n",
    "            values = shap_vals[0, :, pred_class]\n",
    "\n",
    "        return np.abs(values), pred_class\n",
    "\n",
    "    (values, pred_class), t_shap = timed(_compute)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.bar(FEATURE_NAMES, values)\n",
    "    ax.set_title(f\"Variable Contributions to {RISK_MAP[pred_class]}\")\n",
    "    ax.set_ylabel(\"Risk Score\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, f\"{t_shap:.2f} ms\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Maternal Health Risk Prediction Dashboard - XGBoost\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            age = gr.Number(label=\"Age\", value=30)\n",
    "            sbp = gr.Number(label=\"Systolic Blood Pressure\", value=120)\n",
    "            dbp = gr.Number(label=\"Diastolic Blood Pressure\", value=80)\n",
    "            sugar = gr.Number(label=\"Blood Sugar (mmol/L)\", value=7.0)\n",
    "            temp = gr.Number(label=\"Body Temperature (¬∞F)\", value=98.6)\n",
    "            hr = gr.Number(label=\"Heart Rate\", value=75)\n",
    "            btn = gr.Button(\"üîç Predict Risk\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            risk_out = gr.Label(label=\"Risk Level\")\n",
    "\n",
    "            metrics_out = gr.Textbox(\n",
    "                label=\"Model Metrics\",\n",
    "                interactive=False,\n",
    "                lines=4,\n",
    "                max_lines=4,\n",
    "            )\n",
    "\n",
    "            shap_plot = gr.Plot(label=\"Risk Factor Graph\")\n",
    "\n",
    "            shap_time = gr.Textbox(\n",
    "                label=\"SHAP Computation Time\",\n",
    "                interactive=False,\n",
    "                lines=1,\n",
    "                max_lines=1,\n",
    "            )\n",
    "\n",
    "    btn.click(\n",
    "        predict,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[risk_out, metrics_out, shap_time, shap_plot, shap_time],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        compute_shap,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[shap_plot, shap_time],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cab66-54e7-4941-ba92-cd86249a2cfd",
   "metadata": {},
   "source": [
    "### V2: TabPFN (Kernel SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f60f5fe6-897f-4308-95a8-8fc89de0b9ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:08:11.964017Z",
     "iopub.status.busy": "2026-01-28T13:08:11.963021Z",
     "iopub.status.idle": "2026-01-28T13:08:13.322650Z",
     "shell.execute_reply": "2026-01-28T13:08:13.320649Z",
     "shell.execute_reply.started": "2026-01-28T13:08:11.964017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app.py\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import torch\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SHAP_TIMEOUT_SECONDS = 1000\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"Age\",\n",
    "    \"Systolic BP\",\n",
    "    \"Diastolic BP\",\n",
    "    \"Blood Sugar (mmol/L)\",\n",
    "    \"Temperature (¬∞F)\",\n",
    "    \"Heart Rate\",\n",
    "]\n",
    "\n",
    "RISK_MAP = {0: \"Low Risk\", 1: \"Mid Risk\", 2: \"High Risk\"}\n",
    "\n",
    "\n",
    "def format_input(age, sbp, dbp, sugar, temp, hr):\n",
    "    return np.array([[age, sbp, dbp, sugar, temp, hr]], dtype=float)\n",
    "\n",
    "\n",
    "def timed(fn):\n",
    "    start = time.perf_counter()\n",
    "    out = fn()\n",
    "    return out, (time.perf_counter() - start) * 1000\n",
    "\n",
    "\n",
    "# ------------------ MODEL ------------------\n",
    "model = joblib.load(\"final_tabpfn_model.pkl\")\n",
    "\n",
    "\n",
    "def predict(age, sbp, dbp, sugar, temp, hr):\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "\n",
    "    def _predict():\n",
    "        probs = model.predict_proba(X)[0]\n",
    "        return RISK_MAP[int(np.argmax(probs))]\n",
    "\n",
    "    risk, t_pred = timed(_predict)\n",
    "\n",
    "    metrics = (\n",
    "        \"Model Accuracy: 82.24%\\n\"\n",
    "        \"F1 Score: 82.39%\\n\\n\"\n",
    "        f\"Inference Time: {t_pred:.2f} ms\"\n",
    "    )\n",
    "\n",
    "    return risk, metrics, \"Calculating...\", None, \"Calculating...\"\n",
    "\n",
    "\n",
    "# ------------------ SHAP (Kernel + Timeout) ------------------\n",
    "def _shap_worker(X, queue):\n",
    "    try:\n",
    "        background = np.zeros((20, X.shape[1]))\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "        shap_vals = explainer.shap_values(X, nsamples=100)\n",
    "        pred_class = int(np.argmax(model.predict_proba(X)))\n",
    "        values = np.abs(shap_vals[pred_class][0])\n",
    "        queue.put((values, pred_class))\n",
    "    except Exception:\n",
    "        queue.put(None)  # ensure queue always returns\n",
    "\n",
    "\n",
    "def compute_shap(age, sbp, dbp, sugar, temp, hr):\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "    queue = mp.Queue()\n",
    "    process = mp.Process(target=_shap_worker, args=(X, queue))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    process.start()\n",
    "    process.join(timeout=SHAP_TIMEOUT_SECONDS)\n",
    "\n",
    "    if process.is_alive():\n",
    "        process.terminate()\n",
    "        process.join()\n",
    "        return None, f\"Stopped (>{SHAP_TIMEOUT_SECONDS}s timeout)\"\n",
    "\n",
    "    # Check if worker returned results\n",
    "    if queue.empty():\n",
    "        return None, f\"Stopped (>{SHAP_TIMEOUT_SECONDS}s timeout)\"\n",
    "\n",
    "    values_pred = queue.get()\n",
    "    if values_pred is None:\n",
    "        return None, f\"Stopped (>{SHAP_TIMEOUT_SECONDS}s timeout)\"\n",
    "\n",
    "    values, pred_class = values_pred\n",
    "    t_shap = (time.perf_counter() - start) * 1000\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.bar(FEATURE_NAMES, values)\n",
    "    ax.set_title(f\"Variable Contributions to {RISK_MAP[pred_class]}\")\n",
    "    ax.set_ylabel(\"Impact\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, f\"{t_shap:.2f} ms\"\n",
    "\n",
    "\n",
    "# ------------------ UI ------------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Maternal Health Risk Prediction Dashboard - TabPFN Kernel SHAP\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            age = gr.Number(label=\"Age\", value=30)\n",
    "            sbp = gr.Number(label=\"Systolic Blood Pressure\", value=120)\n",
    "            dbp = gr.Number(label=\"Diastolic Blood Pressure\", value=80)\n",
    "            sugar = gr.Number(label=\"Blood Sugar (mmol/L)\", value=7.0)\n",
    "            temp = gr.Number(label=\"Body Temperature (¬∞F)\", value=98.6)\n",
    "            hr = gr.Number(label=\"Heart Rate\", value=75)\n",
    "            btn = gr.Button(\"üîç Predict Risk\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            risk_out = gr.Label(label=\"Risk Level\")\n",
    "\n",
    "            metrics_out = gr.Textbox(\n",
    "                label=\"Model Metrics\",\n",
    "                interactive=False,\n",
    "                lines=4,\n",
    "                max_lines=4,\n",
    "            )\n",
    "\n",
    "            shap_plot = gr.Plot(label=\"Risk Factor Graph\")\n",
    "\n",
    "            shap_time = gr.Textbox(\n",
    "                label=\"SHAP Computation Time\",\n",
    "                interactive=False,\n",
    "                lines=1,\n",
    "                max_lines=1,\n",
    "            )\n",
    "\n",
    "    btn.click(\n",
    "        predict,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[risk_out, metrics_out, shap_time, shap_plot, shap_time],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        compute_shap,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[shap_plot, shap_time],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d0299-6dcd-4c3e-9179-b540cd994186",
   "metadata": {},
   "source": [
    "### V3: XGBoost-TabPFN Weighted Soft-voting Ensemble Model (Kernel SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f9221ff-dd50-4ca9-8978-9b9be2e64356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:07:43.586414Z",
     "iopub.status.busy": "2026-01-28T13:07:43.586414Z",
     "iopub.status.idle": "2026-01-28T13:07:44.960585Z",
     "shell.execute_reply": "2026-01-28T13:07:44.957586Z",
     "shell.execute_reply.started": "2026-01-28T13:07:43.586414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app.py\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "SHAP_TIMEOUT_SECONDS = 1000\n",
    "MODEL_PATH = \"final_weighted_ensemble.pkl\"\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"Age\",\n",
    "    \"Systolic BP\",\n",
    "    \"Diastolic BP\",\n",
    "    \"Blood Sugar (mmol/L)\",\n",
    "    \"Temperature (¬∞F)\",\n",
    "    \"Heart Rate\",\n",
    "]\n",
    "\n",
    "RISK_MAP = {0: \"Low Risk\", 1: \"Mid Risk\", 2: \"High Risk\"}\n",
    "\n",
    "\n",
    "def format_input(age, sbp, dbp, sugar, temp, hr):\n",
    "    return np.array([[age, sbp, dbp, sugar, temp, hr]], dtype=float)\n",
    "\n",
    "\n",
    "def timed(fn):\n",
    "    start = time.perf_counter()\n",
    "    out = fn()\n",
    "    return out, (time.perf_counter() - start) * 1000\n",
    "\n",
    "\n",
    "# ------------------ MODEL ------------------\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "\n",
    "def predict(age, sbp, dbp, sugar, temp, hr):\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "\n",
    "    def _predict():\n",
    "        probs = model.predict_proba(X)[0]\n",
    "        return RISK_MAP[int(np.argmax(probs))]\n",
    "\n",
    "    risk, t_pred = timed(_predict)\n",
    "\n",
    "    metrics = (\n",
    "        \"Model: XGBoost‚ÄìTabPFN Weighted Soft-Voting Ensemble\\n\"\n",
    "        \"Accuracy: 89.16%\\n\"\n",
    "        \"F1 Score: 89.48%\\n\\n\"\n",
    "        f\"Inference Time: {t_pred:.2f} ms\"\n",
    "    )\n",
    "\n",
    "    return risk, metrics, \"Calculating...\", None, \"Calculating...\"\n",
    "\n",
    "\n",
    "# ------------------ SHAP (Kernel + Timeout) ------------------\n",
    "def _shap_worker(model_path, X, queue):\n",
    "    try:\n",
    "        model_local = joblib.load(model_path)\n",
    "        background = np.zeros((20, X.shape[1]))\n",
    "\n",
    "        explainer = shap.KernelExplainer(\n",
    "            model_local.predict_proba,\n",
    "            background\n",
    "        )\n",
    "\n",
    "        shap_vals = explainer.shap_values(X, nsamples=100)\n",
    "        pred_class = int(np.argmax(model_local.predict_proba(X)))\n",
    "\n",
    "        values = np.abs(shap_vals[pred_class][0])\n",
    "        queue.put((values, pred_class))\n",
    "    except Exception:\n",
    "        queue.put(None)\n",
    "\n",
    "\n",
    "def compute_shap(age, sbp, dbp, sugar, temp, hr):\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "    queue = mp.Queue()\n",
    "\n",
    "    process = mp.Process(\n",
    "        target=_shap_worker,\n",
    "        args=(MODEL_PATH, X, queue)\n",
    "    )\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    process.start()\n",
    "    process.join(timeout=SHAP_TIMEOUT_SECONDS)\n",
    "\n",
    "    if process.is_alive() or queue.empty():\n",
    "        process.terminate()\n",
    "        process.join()\n",
    "        return None, f\"Stopped (>{SHAP_TIMEOUT_SECONDS}s timeout)\"\n",
    "\n",
    "    result = queue.get()\n",
    "    if result is None:\n",
    "        return None, f\"Stopped (>{SHAP_TIMEOUT_SECONDS}s timeout)\"\n",
    "\n",
    "    values, pred_class = result\n",
    "    t_shap = (time.perf_counter() - start) * 1000\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.bar(FEATURE_NAMES, values)\n",
    "    ax.set_title(f\"Variable Contributions to {RISK_MAP[pred_class]}\")\n",
    "    ax.set_ylabel(\"Impact\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, f\"{t_shap:.2f} ms\"\n",
    "\n",
    "\n",
    "# ------------------ UI ------------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"# Maternal Health Risk Prediction Dashboard\\n\"\n",
    "        \"**Model:** XGBoost‚ÄìTabPFN Weighted Soft-Voting Ensemble (Kernel SHAP)\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            age = gr.Number(label=\"Age\", value=30)\n",
    "            sbp = gr.Number(label=\"Systolic Blood Pressure\", value=120)\n",
    "            dbp = gr.Number(label=\"Diastolic Blood Pressure\", value=80)\n",
    "            sugar = gr.Number(label=\"Blood Sugar (mmol/L)\", value=7.0)\n",
    "            temp = gr.Number(label=\"Body Temperature (¬∞F)\", value=98.6)\n",
    "            hr = gr.Number(label=\"Heart Rate\", value=75)\n",
    "            btn = gr.Button(\"üîç Predict Risk\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            risk_out = gr.Label(label=\"Risk Level\")\n",
    "            metrics_out = gr.Textbox(\n",
    "                label=\"Model Metrics\",\n",
    "                interactive=False,\n",
    "                lines=5,\n",
    "                max_lines=6,\n",
    "            )\n",
    "            shap_plot = gr.Plot(label=\"Risk Factor Graph\")\n",
    "            shap_time = gr.Textbox(\n",
    "                label=\"SHAP Computation Time\",\n",
    "                interactive=False,\n",
    "                lines=1,\n",
    "                max_lines=1,\n",
    "            )\n",
    "\n",
    "    btn.click(\n",
    "        predict,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[risk_out, metrics_out, shap_time, shap_plot, shap_time],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        compute_shap,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[shap_plot, shap_time],\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691a1df-c813-4803-8f3b-80be0e28392d",
   "metadata": {},
   "source": [
    "### V4: Weighted Soft-voting Ensemble Model (k-means Lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe846ea-c97f-4939-a042-57935db75709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T12:35:49.881671Z",
     "iopub.status.busy": "2026-01-28T12:35:49.880658Z",
     "iopub.status.idle": "2026-01-28T12:35:51.318018Z",
     "shell.execute_reply": "2026-01-28T12:35:51.317015Z",
     "shell.execute_reply.started": "2026-01-28T12:35:49.881671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# ============================================================\n",
    "# RESTORED CLASS ‚Äî MUST MATCH SAVED VERSION EXACTLY\n",
    "# ============================================================\n",
    "\n",
    "class WeightedEnsembleModel:\n",
    "    def __init__(self, models, weights):\n",
    "        self.models = models\n",
    "        self.weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_classes = self.models[0][1].predict_proba(X).shape[1]\n",
    "        proba_sum = np.zeros((X.shape[0], n_classes))\n",
    "        for (_, model), w in zip(self.models, self.weights):\n",
    "            proba_sum += model.predict_proba(X) * w\n",
    "        return proba_sum\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "CSV_FEATURE_COLUMNS = [\n",
    "    \"Age\", \"SystolicBP\", \"DiastolicBP\",\n",
    "    \"BS\", \"BodyTemp\", \"HeartRate\",\n",
    "]\n",
    "\n",
    "UI_FEATURE_NAMES = [\n",
    "    \"Age\", \"Systolic BP\", \"Diastolic BP\",\n",
    "    \"Blood Sugar\", \"Temperature\", \"Heart Rate\",\n",
    "]\n",
    "\n",
    "RISK_MAP = {0: \"üü¢ Low Risk\", 1: \"üü° Mid Risk\", 2: \"üî¥ High Risk\"}\n",
    "\n",
    "MODEL_PATH = \"final_weighted_ensemble.pkl\"\n",
    "SHAP_LIBRARY_PATH = \"shap_library_ensemble.pkl\"\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL + SHAP LIBRARY\n",
    "# ============================================================\n",
    "\n",
    "ensemble = joblib.load(MODEL_PATH)\n",
    "\n",
    "shap_lib = joblib.load(SHAP_LIBRARY_PATH)\n",
    "CLUSTER_CENTERS = shap_lib[\"centers\"]\n",
    "SHAP_VALUES = shap_lib[\"shap_values\"]\n",
    "PRED_CLASSES = shap_lib[\"pred_classes\"]\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def format_input(age, sbp, dbp, sugar, temp, hr):\n",
    "    return pd.DataFrame([[age, sbp, dbp, sugar, temp, hr]],\n",
    "                         columns=CSV_FEATURE_COLUMNS)\n",
    "\n",
    "def nearest_cluster(X):\n",
    "    return pairwise_distances_argmin(X.values, CLUSTER_CENTERS)[0]\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION\n",
    "# ============================================================\n",
    "\n",
    "def predict(age, sbp, dbp, sugar, temp, hr):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "    risk_idx = int(ensemble.predict(X)[0])\n",
    "\n",
    "    inference_ms = (time.perf_counter() - start) * 1000\n",
    "\n",
    "    metrics_md = f\"\"\"\n",
    "### üìä Model Performance  \n",
    "**Weighted Ensemble (XGBoost + TabPFN)**  \n",
    "\n",
    "- **Accuracy:** `0.8916`  \n",
    "- **F1 Score:** `0.8947`  \n",
    "- **Inference Time:** `{inference_ms:.2f} ms`\n",
    "\"\"\"\n",
    "\n",
    "    return RISK_MAP[risk_idx], metrics_md, f\"{inference_ms:.2f} ms\", None, \"‚Äî\"\n",
    "\n",
    "# ============================================================\n",
    "# SHAP (PRECOMPUTED ‚Äî TRUE WALL TIME)\n",
    "# ============================================================\n",
    "\n",
    "def compute_shap(age, sbp, dbp, sugar, temp, hr):\n",
    "    start_total = time.perf_counter()\n",
    "\n",
    "    X = format_input(age, sbp, dbp, sugar, temp, hr)\n",
    "    idx = nearest_cluster(X)\n",
    "\n",
    "    pred_class = PRED_CLASSES[idx]\n",
    "    arr = np.squeeze(SHAP_VALUES[idx])\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.mean(np.abs(arr), axis=1)\n",
    "\n",
    "    # ---------- PLOT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 2.8))\n",
    "    bars = ax.bar(UI_FEATURE_NAMES, arr)\n",
    "\n",
    "    max_val = float(np.max(arr))\n",
    "    padding = max_val * 0.15\n",
    "    ax.set_ylim(0, max_val + padding)\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            height + padding * 0.05,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "            clip_on=True,\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Risk Contribution\")\n",
    "    ax.set_ylabel(\"SHAP Score\")\n",
    "    plt.xticks(rotation=25, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    total_ms = (time.perf_counter() - start_total) * 1000\n",
    "\n",
    "    return fig, f\"{total_ms:.2f} ms\"\n",
    "\n",
    "# ============================================================\n",
    "# UI\n",
    "# ============================================================\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \"# Maternal Health Risk Prediction Dashboard\\n\"\n",
    "    \"**Model:** XGBoost‚ÄìTabPFN Weighted Soft-Voting Ensemble (k-means Lookup)\"\n",
    "    \n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### üßç Patient Inputs\")\n",
    "            age = gr.Number(label=\"Age\", value=30)\n",
    "            sbp = gr.Number(label=\"Systolic BP\", value=120)\n",
    "            dbp = gr.Number(label=\"Diastolic BP\", value=80)\n",
    "            sugar = gr.Number(label=\"Blood Sugar (mmol/L)\", value=7.0)\n",
    "            temp = gr.Number(label=\"Temperature (¬∞F)\", value=98.6)\n",
    "            hr = gr.Number(label=\"Heart Rate\", value=75)\n",
    "            btn = gr.Button(\"üîç Predict Risk\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            risk_out = gr.Label(label=\"Risk Level\")\n",
    "            metrics_out = gr.Markdown()\n",
    "            shap_plot = gr.Plot(label=\"Risk Factor Contribution\")\n",
    "            shap_time = gr.Textbox(label=\"SHAP End-to-End Time\", interactive=False)\n",
    "\n",
    "    btn.click(\n",
    "        predict,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[risk_out, metrics_out, shap_time, shap_plot, shap_time],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        compute_shap,\n",
    "        inputs=[age, sbp, dbp, sugar, temp, hr],\n",
    "        outputs=[shap_plot, shap_time],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93ce5a-6fcb-4a0e-88b6-0d80acfb5362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
